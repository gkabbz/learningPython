Spark Data Frame Basics

Default Imports

    - from pyspark.sql import SparkSession:- To enable creating a spark session
    - from pyspark.sql.types import DateType, StructType, StructField, IntegerType, StringType, DecimalType, LongType:-  StructField and StructType are useful for defining schemas. The other types are useful for defining dates, strings, integer etc.
    - from datetime import datetime:- To help with working on dates and time
    - import pyspark.sql.functions as function
        - useful functions: (col, udf, unix_timestamp, dayofmonth, hour, dayofyear, month, year, weekofyear, format_number, date_format,date_add, datediff, to_date, lit)


1. Starting a Spark Session

    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName('selectedName').getOrCreate()  ## to start the session

2. Reading a dataset into a spark session

Reading Parquet files

    df = spark.read.parquet('s3://net-mozaws-prod-us-west-2-pipeline-data/telemetry-new-profile-parquet/v1/') ## if you tab after read. it will show you options available. If reading from directories, spark is finicky working with file paths with spaces so you may need to get rid of them.

Due to how Parquet files are updated, it may be beneficial to always merge schemas

    df = spark.read.option('mergeSchema', 'true').parquet('s3://net-mozaws-prod-us-west-2-pipeline-data/telemetry-new-profile-parquet/v1/')

Reading CSV Files

    df = spark.read.csv('') ## used for reading a single csv or distributed file csvs stored within a folder

3. Options while reading a dataset into a spark session

    df = spark.read.csv('/folder/folder/file.csv', header = True/False, inferSchema=True/False, schema=schemaVariable)
        inferSchema - read data and infer schema structure
        header - whether a header is included in the file or if it isn't (false)
        schema - define a specific schema for the data

    Defining a schema

    Sometimes data schema may not be in good form so you have to define it e.g. everything is saved as a string.
    a) import type tools: from pyspark.sql.types import StructField, StringType, IntegerType, StructType
    b) create list of structure fields - expected schema
        schemaVariable = StructType([StructField('age', IntegerType(), True), (StructField('column2', StringType(), True))]) - True boolean is to allow whether a field can be Null
    c) Clarify you want the data frame to use the defined schema structure
        df = spark.read.parquet('s3://net-mozaws-prod-us-west-2-pipeline-data/telemetry-new-profile-parquet/v1/', schema=schemaVariable)


4. Reading the dataFrame

    - show the data in the frame:       df.show(5)
    - show the schema:                  df.printSchema()
    - show the column names:            df.columns      ## attribute so no need for ()
    - get statistical summary of df:    df.describe().show ## statistical summary of the numeric columns in the dataframe

5. Selecting vs Grabbing Data

a) Working with columns
    - Returning a column object:                                        df['age']
    - Returning a column in a dataframe (more flexibility):             df.select('age')
    - Select multiple columns in a dataframe and show:                  df.select(['age', 'name']).show
    - Creating a new column:                                            df.withColumn('newage',df[age])
    - Renaming a column:                                                df.withColumnRenamed('age', 'myNewAge')

b) Working with rows
    df.head(2) - returns the first two rows in a dataframe with a list that you can index
    df.head(2)[0] - returns the first row of the two selected above

c) Using SQL with dataframes
    Register the dataframe as a sql temporary view:                     df.createOrReplaceTempView('people')
    Query the temporary view:                                           results = spark.sql("SELECT * FROM people")

d) Filter data
- Filter a column by specific values (similar to sql):        df.filter("submission_date > 20170101").select('client_id','sap').show()
- Filter a column using python operators (way to do it):      df.filter(['submission_date'] > 20170101).select('client_id', sap).show()
- Filtering based on multiple conditions:                     df.filter((df['field1'] > 10) & (df['field2'] = 5)).select().show()
  ~ = not , & = and, | = or, == = equal
- Grab a specific row instance:                               df.filter(df['low'] == 197.16).show()
- To collect data:                                            results = df.filter(df['low'] == 197.16).collect()
- Select Max, Min, Avg, Median in column:                     df.agg({'colName' : 'max'})
- Filter for items contained in a list                         List : last5Versions = [55,56,57,58,59]
                                                                      aDAURecent5Versions = aDAUVersion.filter(aDAUVersion.max_app_version_trunc.isin(last5Versions))

e) Group By and Aggregate Functions
First you need to make sure the functions you want to use have been imported

    Importing functions:                  from pyspark.sql.functions import countDistinct,avg,stddev (if you tab after import you'll see a list of functions available)
[
Grouping Data
    Group by a dimension:                           df.groupBy("Company")
    Group by a dimension and aggregate:             df.groupBy("Company").agg(sum(df.revenue).alias('revenue'), countDistinct(df.clientID).alias('customers'))

Functions


    Summing across rows:                            df.agg({'sales':'sum'}).show()


    Count distinct:                                 df.select(countDistinct('sales')).show()
    To alias:                                       df.select(countDistinct('sales').alias('average sales')).show()

    Order by column:                                df.orderBy("sales").show()
    Order by descending:                            df.orderBy(df['sales'].desc()).show()

    Formatting data to remove decimals:             1. Import format_number from pyspark.sql.functions
                                                    2. avgSales = df.select(countDistinct('sales').alias('average sales'))
                                                    3. avgSales.select(format_number('average sales',2).show() - to 2 decimal places
-
f) Dates and TimeStamps

    importing functions:                            from pyspark.sql.functions import (dayofmonth, hour, dayofyear, month, year, weekofyear,format_number, date_format,date_add, datediff)
    working with dates:                             df.select(year(df['dateColumn'])).show()

Converting Date Stored as String to Date Format
    1. Create a function
        func = udf(lambda x: datetime.strptime(str(x), '%Y%m%d'), DateType())
    2. Apply the function to a specific column in your dataframe
    cleanedDates = df.withColumn('date', func(col('submissionDateS3')))


g) Getting csvs from spark to local

 Not Using an S3 Bucket
    1. testdf.write.csv('s3://net-mozaws-prod-us-west-2-pipeline-analysis/palomar201804/data.csv',header=True)
    2. In atmo node terminal: aws s3 sync s3://net-mozaws-prod-us-west-2-pipeline-analysis/gkabbz/weeklyReporting/Dimensioned/20190307-20190310.csv /home/hadoop/sparkAnalysis/weeklyReporting/dimensioned/20190307-20190310
    3. On local terminal: rsync -av gkabbz-001:/home/hadoop/sparkAnalysis/weeklyReporting/dimensioned/20190307-20190310 /Users/gkaberere/spark-warehouse/weeklyReporting/dimensionedFinal (gkabbz-ltv-02 = atmo node ssh name, /users/gkaberere/spark-warehouse = folder I want to save to)



Transfer file from local to ec2
-- uploading code
rsync -av /Users/gkaberere/Google\ Drive/Github/marketing-analytics gkabbz-001:/home/hadoop/sparkAnalysis/code
-- uploading files
rsync -av /users/gkaberere/spark-warehouse/testSnippet/testSnippet/snippetsNightlyTestDataNov19-25.csv gkabbz-001:/home/hadoop/sparkAnalysis/testSnippet
aws s3 sync /home/hadoop/sparkAnalysis/testSnippet s3://net-mozaws-prod-us-west-2-pipeline-analysis/gkabbz/testSnippet

gsutil cp /Users/gkaberere/spark-warehouse/weeklyReporting/dimensionedFinal/20190307-20190310Cleaned.csv gs://gkabbz-upload

rsync -av /users/gkaberere/spark-warehouse/testSnippet/testSnippet gkabbz-001:/home/hadoop/sparkAnalysis/testSnippet
Using an S3 Bucket


h) Understanding a table
- Get number of rows in a dataframe:                print len(df.columns)
- Get number of partitions:                         df.rdd.getNumPartitions()
- Get number of rows:                               df.count()

To Do Items
TODO 1. How to calculate a max date from a column and pass it to a new column
TODO 2. Add how to persist() results for faster access
TODO 3. Go through Jupyter notebooks and make sure all tests and progress have been documented here
TODO 4. Naming conventions for dataframes
TODO 5. Syntax for joins
TODO 6. Similar statement to where x is in ('a', 'b', 'c') when filtering


rsync -av gkabbz-001:/home/hadoop/sparkAnalysis/code /Users/gkaberere/Google\ Drive/Github/marketing-analytics

Not Using an S3 Bucket
    1. testdf.write.csv('s3://net-mozaws-prod-us-west-2-pipeline-analysis/palomar201804/data.csv',header=True)
    2. In atmo node terminal: aws s3 sync s3://net-mozaws-prod-us-west-2-pipeline-analysis/gkabbz/testSnippet/testJoin3.csv /home/hadoop/sparkAnalysis/testSnippet
    3. On local terminal: rsync -av gkabbz-001:/home/hadoop/sparkAnalysis/testSnippet /Users/gkaberere/spark-warehouse/testSnippet (gkabbz-ltv-02 = atmo node ssh name, /users/gkaberere/spark-warehouse = folder I want to save